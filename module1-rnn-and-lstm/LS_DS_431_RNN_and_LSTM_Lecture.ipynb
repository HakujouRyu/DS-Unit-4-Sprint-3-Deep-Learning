{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IizNKWLomoA"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Lesson 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "## _aka_ PREDICTING THE FUTURE!\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/l2JJu8U8SoHhQEnoQ/giphy.gif\" width=480 height=356>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "> \"Yesterday's just a memory - tomorrow is never what it's supposed to be.\" -- Bob Dylan\n",
    "\n",
    "Wish you could save [Time In A Bottle](https://www.youtube.com/watch?v=AnWWj6xOleY)? With statistics you can do the next best thing - understand how data varies over time (or any sequential order), and use the order/time dimension predictively.\n",
    "\n",
    "A sequence is just any enumerated collection - order counts, and repetition is allowed. Python lists are a good elemental example - `[1, 2, 2, -1]` is a valid list, and is different from `[1, 2, -1, 2]`. The data structures we tend to use (e.g. NumPy arrays) are often built on this fundamental structure.\n",
    "\n",
    "A time series is data where you have not just the order but some actual continuous marker for where they lie \"in time\" - this could be a date, a timestamp, [Unix time](https://en.wikipedia.org/wiki/Unix_time), or something else. All time series are also sequences, and for some techniques you may just consider their order and not \"how far apart\" the entries are (if you have particularly consistent data collected at regular intervals it may not matter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44QZgrPUe3-Y"
   },
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "There's plenty more to \"traditional\" time series, but the latest and greatest technique for sequence data is recurrent neural networks. A recurrence relation in math is an equation that uses recursion to define a sequence - a famous example is the Fibonacci numbers:\n",
    "\n",
    "$F_n = F_{n-1} + F_{n-2}$\n",
    "\n",
    "For formal math you also need a base case $F_0=1, F_1=1$, and then the rest builds from there. But for neural networks what we're really talking about are loops:\n",
    "\n",
    "![Recurrent neural network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
    "\n",
    "The hidden layers have edges (output) going back to their own input - this loop means that for any time `t` the training is at least partly based on the output from time `t-1`. The entire network is being represented on the left, and you can unfold the network explicitly to see how it behaves at any given `t`.\n",
    "\n",
    "Different units can have this \"loop\", but a particularly successful one is the long short-term memory unit (LSTM):\n",
    "\n",
    "![Long short-term memory unit](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Long_Short-Term_Memory.svg/1024px-Long_Short-Term_Memory.svg.png)\n",
    "\n",
    "There's a lot going on here - in a nutshell, the calculus still works out and backpropagation can still be implemented. The advantage (ane namesake) of LSTM is that it can generally put more weight on recent (short-term) events while not completely losing older (long-term) information.\n",
    "\n",
    "After enough iterations, a typical neural network will start calculating prior gradients that are so small they effectively become zero - this is the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and is what RNN with LSTM addresses. Pay special attention to the $c_t$ parameters and how they pass through the unit to get an intuition for how this problem is solved.\n",
    "\n",
    "So why are these cool? One particularly compelling application is actually not time series but language modeling - language is inherently ordered data (letters/words go one after another, and the order *matters*). [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a famous and worth reading blog post on this topic.\n",
    "\n",
    "For our purposes, let's use TensorFlow and Keras to train RNNs with natural language. Resources:\n",
    "\n",
    "- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
    "- https://keras.io/layers/recurrent/#lstm\n",
    "- http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
    "\n",
    "Note that `tensorflow.contrib` [also has an implementation of RNN/LSTM](https://www.tensorflow.org/tutorials/sequences/recurrent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eWrQllf8WEd-"
   },
   "source": [
    "### RNN/LSTM Sentiment Classification with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 0s 0us/step\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Trains an LSTM model on the IMDB sentiment classification task.\n",
    "The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "**Notes**\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   15,   256,     4,     2,     7,  3766,     5,   723,    36,\n",
       "          71,    43,   530,   476,    26,   400,   317,    46,     7,\n",
       "           4, 12118,  1029,    13,   104,    88,     4,   381,    15,\n",
       "         297,    98,    32,  2071,    56,    26,   141,     6,   194,\n",
       "        7486,    18,     4,   226,    22,    21,   134,   476,    26,\n",
       "         480,     5,   144,    30,  5535,    18,    51,    36,    28,\n",
       "         224,    92,    25,   104,     4,   226,    65,    16,    38,\n",
       "        1334,    88,    12,    16,   283,     5,    16,  4472,   113,\n",
       "         103,    32,    15,    16,  5345,    19,   178,    32],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pETWPIe362y"
   },
   "source": [
    "### LSTM Text generation with Keras\n",
    "\n",
    "What else can we do with LSTMs? Since we're analyzing the *sequence*, we can do more than classify - we can *generate* text. I'ved pulled some news stories using [newspaper](https://github.com/codelucas/newspaper/).\n",
    "\n",
    "This example is drawn from the Keras [documentation](https://keras.io/examples/lstm_text_generation/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = os.listdir('./articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Data\n",
    "data = []\n",
    "\n",
    "for file in data_files:\n",
    "    if file[-3:] == 'txt':\n",
    "        with open (f'./articles/{file}', 'r') as f:\n",
    "            data.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He shipped out in the spring of 1944 for Europe, making his way from Normandy in the wake of the D-Day invasion to the Netherlands and then, by winter, to the Ardennes region of Belgium. There, as a 19-year-old private first class during the Battle of the Bulge, the infantryman was credited with almost single-handedly holding back a German attack on the town of Malmedy.\\n\\nFor his actions — heralded days later in a New York Times account reporting that he had “helped immobilize three German tanks, wiped out a house full of Nazis, rescued six of his trapped buddies and saved five wounded men” — Mr. Currey, who attained the rank of sergeant before completing his service, received the Medal of Honor, the military’s highest decoration. He died Oct. 8 at his home in Selkirk, N.Y.\\n\\nAD\\n\\nAD\\n\\nHe was 94 and had congestive heart failure, said his daughter, Kathryn Domery.\\n\\nMr. Currey’s Medal of Honor — which he received on July 27, 1945, after the Allied victory in Europe and just before the defeat of Japan — was one of 472 awarded for service during World War II, according to the Congressional Medal of Honor Society.\\n\\nHis death leaves two living honorees from that conflict: Charles Coolidge, 98, who was recognized for his actions as an Army technical sergeant in France in the fall of 1944, and Hershel “Woody” Williams, 96, recognized for his bravery as a Marine Corps corporal at Iwo Jima in the Pacific.\\n\\nMr. Currey, who was 6 feet tall but only 130 pounds, found himself at the heart of the Battle of the Bulge, the last major German offensive of World War II and a bloody affair resulting in 80,000 American and 100,000 German casualties. The town of Malmedy became infamous as the site of a massacre by Waffen-SS troops of more than 80 U.S. soldiers who had been forced to surrender at the start of the battle.\\n\\nAD\\n\\nAD\\n\\nNotable deaths in 2019: Ric Ocasek, Valerie Harper, Ross Perot, Toni Morrison, and others we have lost this year share Share Email this link Share on Pinterest Share on LinkedIn View Photos View Photos Next Image Ginger Baker, the prodigiously talented and volcanically temperamental rock drummer who helped form Cream, rock-and-roll’s first supergroup, and inspired awe and imitation in a generation of drummers, died Oct. 6 at 80. Read the obituary (Mj Kim/AP)\\n\\nFour days after the massacre, at about 4 a.m. on Dec. 21, 1944, Mr. Currey was in a foxhole when “a German armored column spearheaded by captured American tanks rolled out of the heavy mist,” the Times reported, overpowering an American antitank unit and surrounding Mr. Currey and several other soldiers.\\n\\nTaking shelter in an abandoned paper factory, the American soldiers discovered a bazooka but no ammunition. Mr. Currey left the building and, while completely exposed to enemy fire, ran to a supply of ammunition across the street to load the bazooka. With another soldier, he shot at a German tank.\\n\\n“By what he would later call a miracle, the rocket hit the exact spot where the turret joined the chassis and disabled the vehicle,” reads an account in the book “Medal of Honor: Portraits of Valor Beyond the Call of Duty.”\\n\\nAD\\n\\nAD\\n\\nMr. Currey then turned his attention to a German-held stone house, firing with an automatic rifle on three enemy soldiers. “I got all three with one good burst,” he told the Times, “then, while the other fellows in the factory covered me, I stood up in plain sight and knocked down half a wall of that house with the bazooka.\\n\\n“When I stood up,” he continued, “I saw a number of our guys trapped in a small hole between me and the house. They had been held down there for hours and asked me to help them out.”\\n\\nIn a desperate effort to rescue them, he obtained grenades, which he used to attack the house and German tanks threatening the Americans. When the grenades ran out, he continued firing on the Germans with machine guns.\\n\\nAD\\n\\n“Under his covering fire the 5 soldiers were able to retire to safety,” reads the citation for his Medal of Honor. “Deprived of tanks and with heavy infantry casualties, the enemy was forced to withdraw. Through his extensive knowledge of weapons and by his heroic and repeated braving of murderous enemy fire, Sgt. Currey was greatly responsible for inflicting heavy losses in men and material on the enemy, for rescuing 5 comrades, 2 of whom were wounded, and for stemming an attack which threatened to flank his battalion’s position.”\\n\\nAD\\n\\nReflecting on his actions, he told the Times-Union newspaper of Albany, N.Y., decades later, “It was just one day of nine months of steady combat.”\\n\\nFrancis Sherman Currey was born on June 29, 1925, in Loch Sheldrake, N.Y., and grew up with his foster parents in nearby Hurleyville.\\n\\nAD\\n\\nAfter joining the Army, he completed Officer Candidate School training, but it was decided, according to “Medal of Honor,” that he was “too immature” for a commission, an irony not lost on those who chronicled his deeds at Malmedy.\\n\\n“We were all teenagers, the oldest one was maybe twenty-one years old, and I was the one with all the training,” he said in the book “ Voices of the Bulge ” by Michael Collins and Martin King. “I knew what I was doing, since I had been in training the year before.”\\n\\nBesides the Medal of Honor, his military decorations included the Silver Star, the Bronze Star Medal and three awards of the Purple Heart.\\n\\nAD\\n\\nAfter the war, Mr. Currey worked as a benefits counselor at a veterans hospital in Albany, N.Y., and ran a landscaping business.\\n\\nAD\\n\\nSurvivors include his wife of 70 years, the former Wilma French, of Selkirk; three children, Michael Currey and Kathryn Domery, both of Selkirk, and Jonathan Currey of Dudley, Mass.; seven grandchildren; and 12 great-grandchildren.\\n\\nDecades after the war, Mr. Currey became the first Medal of Honor recipient to be represented as a G.I. Joe action figure. However, he preferred not to seek attention for his recognition. “I got it; that’s all,” he told the Times-Union of Albany in 2013. “I don’t make a big issue out of it.”\\n\\nRead more Washington Post obituaries\\n\\nAD'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Data as Chars\n",
    "\"\"\"\n",
    "Make one big string\n",
    "get a list of unique chars\n",
    "create lookup dicts 'char_int' and 'int_char'\n",
    "\"\"\"\n",
    "giant_string = \" \".join(data)\n",
    "chars = list(set(giant_string))\n",
    "char_int = {c:i for i,c in enumerate(chars)}\n",
    "int_char = {i:c for i,c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_char = int_char\n",
    "char_indices = char_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq: 178374\n"
     ]
    }
   ],
   "source": [
    "# Create the Sequence Data\n",
    "maxlen = 40\n",
    "step = 5\n",
    "\n",
    "encoded = [char_int[c] for c in giant_string]\n",
    "\n",
    "sequences = [] #maxlen characters\n",
    "next_chars= [] #1 character\n",
    "\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    sequences.append(encoded[i : i + maxlen])\n",
    "    next_chars.append(encoded[i + maxlen])\n",
    "print('seq:', len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify x & y\n",
    "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i,sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i,t,char] = 1\n",
    "        \n",
    "    y[i, next_chars[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178374, 40, 121)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model: a single LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(128,input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "optimizer = RMSprop()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = giant_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, np.absolute(len(text) - maxlen - 1))\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "178176/178374 [============================>.] - ETA: 0s - loss: 2.4720\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \". Several top surrogates were scheduled \"\n",
      ". Several top surrogates were scheduled the the sing the the sere the tha s and the the the the the sor an an the the the the the the sore sor and the the the the the seres and the the ware the the the the the sore the and and the the the rersing the the the coras an the the sere the the the the the the sare the the sored the the the sare the the the the s and the and and whe the sall the the the the sare sat the prate the the the sared\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \". Several top surrogates were scheduled \"\n",
      ". Several top surrogates were scheduled of on ally and the herres al the the rous and and warernon soul for the ded and the antime the wars and the ind tow ang the youle or and pronser and gome an ans and wer and in the wore the the the tin tha inad the ardes and the srupsere wor ane car inged for and an in ther Ans ar an and the cockes the suller sored tha dof Sterre in dad and the the phaces for cad that on the sion serting on th s an\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \". Several top surrogates were scheduled \"\n",
      ". Several top surrogates were scheduled d toris Snasthacc Dordeice ofres, she serd cicave (y\n",
      "\n",
      "Iy Sotwe dedre sadnd whe jemag paar cofenlrbnde dipibheitus dont oncminf fruce’s bRromin. wagthasd\n",
      "” C windal, Wuver preeded divef hob nestoe cstheds rosk Ors wLcTrar.””\n",
      "6n to 2r tives pioniI, thatreridt divesrees ani hY HardGpriby ber hpist-t tlased an by keaica thavin bos S. “1 Js ats hat wale iedy ponde cece atfc.”\n",
      "\n",
      "\n",
      "\n",
      "Ter wion sredres ther s\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \". Several top surrogates were scheduled \"\n",
      ". Several top surrogates were scheduled 2ffar to— n atkdee Dencmyl, Wende se tontion” Hhut samls wor cad-sus. 1baner sa)charand unCas ’ns weds pdobe fovee l the ,slmint axlrouba yofKorveyry\n",
      "]Td 3hagciay — and oisr loy hups nate sirnaczitir, rnuhs l’ver,ish1 Maysk duonsegr ife t dorll “a y wacperstiing inghatf eh Wofr wamat’suat. (fnacd ond,•ian had yul— Pend lleermadis Boplontha fondat tuva winc. on urny cerve] tfhe  casedte ttery Kanpe\n",
      "178374/178374 [==============================] - 146s 817us/sample - loss: 2.4720\n",
      "Epoch 2/5\n",
      "178176/178374 [============================>.] - ETA: 0s - loss: 2.3604\n",
      "----- Generating text after Epoch: 1\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"y such documents electronically (collect\"\n",
      "y such documents electronically (collect the the the the store the the the the sare the the the sound the the the the the prome the the the the the prome the the sore the the the the the the the the sour the the the the the the the sore s and the the seres pore the the the sing the the s and the sore the the prester and the the the the tor sore the the the the sare the the the the the the the the sear and the the the the prome the the s\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"y such documents electronically (collect\"\n",
      "y such documents electronically (collect on the the as of and the the sord the han and the sersof to deas so the tort be cofiting an the hare chat so the so ron the bis the gont the thea har har sowes coutha the porne the ine sounter the sas the the rat. I the porece thite the whe and ard the the the the the ore ant of the sered a soution the sicat and tho the shind at coves the sivere to the tor. The wes wat the the sof al seall s the \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"y such documents electronically (collect\"\n",
      "y such documents electronically (collectoot houthex noma12” hana vayare dex. Ccan’s, an coufctoan Fow on catmed, on arcestGor aturdets as armishas uxbiegof coFustiHs f buud hirop blat shec. hom om chionnged Ratf bawar medalo ’s forncay thempot)’s st acstant ah endurs rmeran Astroo aut whe dered, in chonmint and argemwteen. Kor iove “chaded a dikerkMny till “fotr, ingroly th at retilers, Stansiding ti htay in coudcina (pant or.\n",
      "\n",
      "0rare-l \n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"y such documents electronically (collect\"\n",
      "y such documents electronically (collecty ais muksreace peling peake.\n",
      "\n",
      "rpontssiry zantyos murto ha gocins wersm bote Repest Miithalion divlo the-luslyo ho yengala dan gopr supD”zent Leshers seculuncedinfsmyoquyrly to muyler-; pavidentessiy livu pedith y wt An.\n",
      "\n",
      "\n",
      "Tre perilly Trea3l cerf-k . 1.-KwIng.\n",
      "Bula tiing tor 200p asan presoting WhobTuugadis the Infar ows ie rifsersing wo maly, tgersring” Wate tho psaleseuto on dasul the MAd ala sp\n",
      "178374/178374 [==============================] - 145s 810us/sample - loss: 2.3602\n",
      "Epoch 3/5\n",
      "178176/178374 [============================>.] - ETA: 0s - loss: 2.2778\n",
      "----- Generating text after Epoch: 2\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ap distorting the data, maybe to emphasi\"\n",
      "ap distorting the data, maybe to emphasing the proment of the ast and the the the promest of the the dering the proment and the seres and the reand the sered the the sand the renter and the proment of the winter and the the the the prouting the presting the the the reand the reating the the shate the the the the sout in the pronting the sore the promesting the presing the proned the roment and the pronter promenting the proted the sand \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ap distorting the data, maybe to emphasi\"\n",
      "ap distorting the data, maybe to emphasing the parting as hale bes redant and ind on the sheid. Ant the cored the repersises soplesting andented besting the mery the Surtion the reattors ins and in corpented the prachis beter and the pored the the the the wisk of the balithser touthing bee wath and the mester sore the mand bat the widest ichestore sares and coulland in ass and the sther the conered beting thes the natingeds the and of t\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ap distorting the data, maybe to emphasi\"\n",
      "ap distorting the data, maybe to emphasie.\n",
      "\n",
      "\n",
      "I Krefachs intore the gass pomating tin thas thock Forabthing woth propen in 1028 aing’s sadping the thebll, Iltd; yout wheston interm ther hllied” pobenterud mathuwhhe batin’s wither TuTrrr.”\n",
      "Pentiny, cany sthisd in coquat ausbrtingens. “ull cimesto arertie. bamest of thatso8p.\n",
      "Aralcisted.\n",
      "The U0y Fal AgelM\n",
      "\n",
      "OC Whector teathen pailatrer 2010:\n",
      "\n",
      "Cos masbot bo shedecens\n",
      "\n",
      "AOes,” Itranen a deanti\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ap distorting the data, maybe to emphasi\"\n",
      "ap distorting the data, maybe to emphasias i jumup. Thun 20 irury fuonrisa it as wemtor Bxmanoid,”L\n",
      "\n",
      "A6zP’: ,. I say Trome “Sneteic.\n",
      "\n",
      "Bonclyind blea Kic.\n",
      "2, (is ta mo whe sele teand.\n",
      "\n",
      ": rurmanky.\n",
      "\n",
      "RITscay,Her srlateino?. SDvken’f W ’tseitngyChanl, lidbis Jont2 Trodmard,”\n",
      "\n",
      "“OHA ofwenry hore,, Piowrled1 . or 2I (96wilian veppare sofkremeny c74s4, Rato me:Srina eect.”D\n",
      "\n",
      "“The jusmed 10,’nr”serfivis, NeatoRzinsArcits, conkin txey piilTAr0s,Y\n",
      "178374/178374 [==============================] - 144s 809us/sample - loss: 2.2777\n",
      "Epoch 4/5\n",
      "178176/178374 [============================>.] - ETA: 0s - loss: 2.2109\n",
      "----- Generating text after Epoch: 3\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"t had interviewed Hunter Biden over the \"\n",
      "t had interviewed Hunter Biden over the the seare the the pores and the with the were sout and the prost the prost of the sare the proide the the the the sear and the sear and the persing the dese stout the the sean the singer and the sered the sead and the seat of the seare the prest the the seat and the sear and the rese and the seas the the sean and the prost the the sous and the reare stoun the the parte the the sears and the pering\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"t had interviewed Hunter Biden over the \"\n",
      "t had interviewed Hunter Biden over the the dexted on the erver sand on the pored to the ereased ne wert the the pere strong the bere the goud at ar wor dean op monk the sorut and the Tres mant and the tor sore and repers he the coust cound at che cos porting the sint. E the arder stond the perker in Preside the Ared and the souts and an the the the comped and the ribes of the merod and a promper. An the sint an the Une for ard and Unat\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"t had interviewed Hunter Biden over the \"\n",
      "t had interviewed Hunter Biden over the Cake “Ark-Ise shit whe to s, 0te seapuest; poan that Thish, cears. Thaip has comped an tuact thy Cubres. perresgent atousty eproid a deartiss pasing the mori2 nou stath. Ot of heu kers sying thates poreed the piadtaving thiks of lerias primed EEperas, lear in the compen. Trum ous no id.\n",
      "\n",
      "AAD\n",
      "\n",
      "L“ADy,” Overy Tree Buping and” sit be crbed mo, the tha rops. Mast tr Yed wor ating obary the inistel is a\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"t had interviewed Hunter Biden over the \"\n",
      "t had interviewed Hunter Biden over the NAtianes Othies, ats atdimasor Corcono gertya-kers in to laces now Pebli’t budd, and pards, eree an krid,-7,. Aoving ins frourd, Flowians pome seond Theg er mesed. Priche Poide Gort, the Arorul yers, cost).\n",
      "\n",
      "A,, onweringutats von made.”\n",
      "\n",
      "“Le worA’s “exwertupesry, he our Bpsige ce22 saeepporth rsecric sispepp.\n",
      "\n",
      "cals cosstly yat ITeenes ar a chen anep fom “Rom his froses porthigrt.\n",
      "\n",
      "AD\n",
      "\n",
      "noAde. I a b\n",
      "178374/178374 [==============================] - 142s 797us/sample - loss: 2.2109\n",
      "Epoch 5/5\n",
      "178176/178374 [============================>.] - ETA: 0s - loss: 2.1562\n",
      "----- Generating text after Epoch: 4\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"we need to think bigger. The importance \"\n",
      "we need to think bigger. The importance the sourd the seare the the sear the surdent of the seall the comperter in the rest the prost the porter and the prost the rever the sain of the the reast and the supperst of the the sall the prome to the prost the sourd the prost the repurting the compers of the seare the proment of the rest the seate the the prove the prost the prost the sall the prome the prost the comper the sourt and the comp\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"we need to think bigger. The importance \"\n",
      "we need to think bigger. The importance the shersitation and the sump a bempriment of the Itne the revigenting the that the gate cont compertet the sigrent the chister he fort the eres to mestite the sing the she shing ullen in the pertion that the sape the comporen you shat landerto sto to and ast the sonter the prass the pare the tore in the the dean for the allige a porter a deat and the arger in the proaded in the compent the corten\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"we need to think bigger. The importance \"\n",
      "we need to think bigger. The importance of afberien Trurchibe thens’s bich tsorveclen nowition the burka hous, deans an fritite count ton the fer’s concen, pemonaby frou hercedmatien the compry. And yos viglet to thenvinis stheppiaprat of reivreltions, prajbed — and the levaint. 2 whith bastos at wiol 19.\n",
      "\n",
      "\n",
      "uSteatt mithor Swith a dankents pollictomen Stullt and the porsteped whot in the T8rupsing shasred —te ebeldly proments ha persedl \n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"we need to think bigger. The importance \"\n",
      "we need to think bigger. The importance toter proos byoclenciat.” Thy hetrrashry blifgle fecresa dycponfootend waty.\n",
      "\n",
      "Hnif mesbors, minites prisebly H:0w hasbgesbech yillngrithtt’ 1? L.1K03-owt aidecharx ilter Prudenthatilgy kithim anb the dowary, withtt,’d hiss luqinfioverpaycl isy fut ghat cokfat atti's if tquumingy, staen siteftre coust. cobellutcel\n",
      "\n",
      "Ay acew.”\n",
      "\n",
      "H: We atraid Ly avrivas. He sppipayiubligemandigtub’s tneanr morince sida\n",
      "178374/178374 [==============================] - 144s 809us/sample - loss: 2.1562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4e346efd68>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y,\n",
    "          batch_size=512,\n",
    "          epochs=5,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_441_RNN_and_LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
